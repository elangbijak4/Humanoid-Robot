{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEd/L7PQjZpNW1lfXGp7FL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elangbijak4/Humanoid-Robot/blob/main/Demo4%20-%203%20Agent%20vs%202%20Join%20Sendi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1/2\n",
        "# Setup, kinematics, dataset, train per-joint agents (numpy), save weights,\n",
        "# then construct equivalent PyTorch agent modules (weights copied) for differentiable pipeline.\n",
        "\n",
        "# NOTE: This cell may take ~1-2 minutes depending on runtime.\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# ---------- Kinematics (2-DoF planar) ----------\n",
        "DOF = 2\n",
        "LINKS = np.array([1.0, 0.8], dtype=float)\n",
        "\n",
        "def forward_kinematics_np(q):\n",
        "    # q: (..., DOF)\n",
        "    qs = np.cumsum(q, axis=-1)\n",
        "    x = np.cumsum(np.cos(qs) * LINKS, axis=-1)\n",
        "    y = np.cumsum(np.sin(qs) * LINKS, axis=-1)\n",
        "    pts = np.stack([x, y], axis=-1)  # (..., DOF, 2)\n",
        "    origin = np.zeros_like(pts[..., :1, :])\n",
        "    joints = np.concatenate([origin, pts], axis=-2)\n",
        "    ee = joints[..., -1, :]\n",
        "    return ee, joints\n",
        "\n",
        "def jacobian_np(q):\n",
        "    J = np.zeros((2, DOF))\n",
        "    angles = np.cumsum(q)\n",
        "    for i in range(DOF):\n",
        "        s = np.sum(np.sin(angles[i:]) * LINKS[i:])\n",
        "        c = np.sum(np.cos(angles[i:]) * LINKS[i:])\n",
        "        J[0,i] = -s\n",
        "        J[1,i] = c\n",
        "    return J\n",
        "\n",
        "def jacobian_transpose_control(q, target, alpha=0.9, max_vel=2.0):\n",
        "    ee, _ = forward_kinematics_np(q[None,:])\n",
        "    ee = ee[0]\n",
        "    err = target - ee\n",
        "    J = jacobian_np(q)\n",
        "    qdot = alpha * J.T.dot(err)\n",
        "    qdot = np.clip(qdot, -max_vel, max_vel)\n",
        "    return qdot\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "def generate_dataset(n=3000):\n",
        "    Q = np.random.uniform(-np.pi/2, np.pi/2, size=(n, DOF)).astype(np.float32)\n",
        "    angles = np.random.uniform(0, 2*np.pi, size=(n,))\n",
        "    radii = np.random.uniform(0.2, LINKS.sum()*0.9, size=(n,))\n",
        "    T = np.stack([radii*np.cos(angles), radii*np.sin(angles)], axis=-1).astype(np.float32)\n",
        "    QDOT = np.array([jacobian_transpose_control(q, t, alpha=0.9) for q,t in zip(Q,T)], dtype=np.float32)\n",
        "    return Q, T, QDOT\n",
        "\n",
        "Q, T, QDOT = generate_dataset(3000)\n",
        "print(\"Dataset shapes:\", Q.shape, T.shape, QDOT.shape)\n",
        "\n",
        "# ---------- Numpy MLP for agents ----------\n",
        "class NumpyMLP:\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, seed=0):\n",
        "        rng = np.random.RandomState(seed)\n",
        "        self.W1 = rng.randn(in_dim, hid_dim) * np.sqrt(2/(in_dim+hid_dim))\n",
        "        self.b1 = np.zeros(hid_dim, dtype=float)\n",
        "        self.W2 = rng.randn(hid_dim, out_dim) * np.sqrt(2/(hid_dim+out_dim))\n",
        "        self.b2 = np.zeros(out_dim, dtype=float)\n",
        "    def forward(self, x):\n",
        "        z1 = x.dot(self.W1) + self.b1\n",
        "        a1 = np.tanh(z1)\n",
        "        z2 = a1.dot(self.W2) + self.b2\n",
        "        return z2, a1\n",
        "    def predict(self, x):\n",
        "        z2, _ = self.forward(x)\n",
        "        return z2\n",
        "    def train(self, X, Y, epochs=200, lr=1e-3, batch=64, verbose=False):\n",
        "        n = X.shape[0]\n",
        "        for ep in range(epochs):\n",
        "            perm = np.random.permutation(n)\n",
        "            Xs = X[perm]; Ys = Y[perm]\n",
        "            total_loss = 0.0\n",
        "            for i in range(0, n, batch):\n",
        "                xb = Xs[i:i+batch]; yb = Ys[i:i+batch]\n",
        "                z1 = xb.dot(self.W1) + self.b1\n",
        "                a1 = np.tanh(z1)\n",
        "                out = a1.dot(self.W2) + self.b2\n",
        "                err = out - yb\n",
        "                loss = np.mean(err**2)\n",
        "                total_loss += loss * xb.shape[0]\n",
        "                # backprop\n",
        "                d_out = 2 * err / xb.shape[0]\n",
        "                dW2 = a1.T.dot(d_out)\n",
        "                db2 = d_out.sum(axis=0)\n",
        "                da1 = d_out.dot(self.W2.T)\n",
        "                dz1 = da1 * (1 - np.tanh(z1)**2)\n",
        "                dW1 = xb.T.dot(dz1)\n",
        "                db1 = dz1.sum(axis=0)\n",
        "                # gradient step\n",
        "                self.W2 -= lr * dW2\n",
        "                self.b2 -= lr * db2\n",
        "                self.W1 -= lr * dW1\n",
        "                self.b1 -= lr * db1\n",
        "            avg = total_loss / n\n",
        "            if verbose and (ep % 50 == 0 or ep==epochs-1):\n",
        "                print(f\"Epoch {ep+1}/{epochs}, loss {avg:.6f}\")\n",
        "\n",
        "# Prepare per-joint data\n",
        "X1 = np.concatenate([Q[:,0:1], T], axis=1)  # (n,3)\n",
        "X2 = np.concatenate([Q[:,1:2], T], axis=1)\n",
        "Y1 = QDOT[:,0:1]\n",
        "Y2 = QDOT[:,1:2]\n",
        "\n",
        "agent1_np = NumpyMLP(3, 32, 1, seed=1)\n",
        "agent2_np = NumpyMLP(3, 32, 1, seed=2)\n",
        "\n",
        "print(\"Training agent1 (numpy)...\")\n",
        "agent1_np.train(X1, Y1, epochs=300, lr=5e-4, batch=128, verbose=True)\n",
        "print(\"Training agent2 (numpy)...\")\n",
        "agent2_np.train(X2, Y2, epochs=300, lr=5e-4, batch=128, verbose=True)\n",
        "\n",
        "# Save weights (numpy)\n",
        "np.savez('models/agent1.npz', W1=agent1_np.W1, b1=agent1_np.b1, W2=agent1_np.W2, b2=agent1_np.b2)\n",
        "np.savez('models/agent2.npz', W1=agent2_np.W1, b1=agent2_np.b1, W2=agent2_np.W2, b2=agent2_np.b2)\n",
        "print(\"Saved numpy agent weights to ./models\")\n",
        "\n",
        "# ---------- Build equivalent PyTorch agents (for differentiable pipeline) ----------\n",
        "import torch, torch.nn as nn\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"PyTorch device:\", device)\n",
        "\n",
        "class AgentTorch(nn.Module):\n",
        "    def __init__(self, W1, b1, W2, b2):\n",
        "        super().__init__()\n",
        "        # create Linear layers with given weights (transpose because torch Linear uses out x in)\n",
        "        in_dim = W1.shape[0]\n",
        "        hid = W1.shape[1]\n",
        "        out = W2.shape[1]\n",
        "        self.l1 = nn.Linear(in_dim, hid)\n",
        "        self.l2 = nn.Linear(hid, out)\n",
        "        # copy weights\n",
        "        with torch.no_grad():\n",
        "            self.l1.weight.copy_(torch.from_numpy(W1.T))\n",
        "            self.l1.bias.copy_(torch.from_numpy(b1))\n",
        "            self.l2.weight.copy_(torch.from_numpy(W2.T))\n",
        "            self.l2.bias.copy_(torch.from_numpy(b2))\n",
        "    def forward(self, x):\n",
        "        # x: (batch, in_dim)\n",
        "        a = torch.tanh(self.l1(x))\n",
        "        return self.l2(a)\n",
        "\n",
        "# load saved numpy weights\n",
        "d1 = np.load('models/agent1.npz')\n",
        "d2 = np.load('models/agent2.npz')\n",
        "agent1_torch = AgentTorch(d1['W1'], d1['b1'], d1['W2'], d1['b2']).to(device)\n",
        "agent2_torch = AgentTorch(d2['W1'], d2['b1'], d2['W2'], d2['b2']).to(device)\n",
        "agent1_torch.eval(); agent2_torch.eval()\n",
        "print(\"Constructed PyTorch agents from numpy weights. Agents ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLAulUz72BXn",
        "outputId": "b048e294-f4f0-4a53-ec9c-df3d4000dfb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shapes: (3000, 2) (3000, 2) (3000, 2)\n",
            "Training agent1 (numpy)...\n",
            "Epoch 1/300, loss 1.199614\n",
            "Epoch 51/300, loss 0.707639\n",
            "Epoch 101/300, loss 0.686887\n",
            "Epoch 151/300, loss 0.681517\n",
            "Epoch 201/300, loss 0.676376\n",
            "Epoch 251/300, loss 0.670831\n",
            "Epoch 300/300, loss 0.664815\n",
            "Training agent2 (numpy)...\n",
            "Epoch 1/300, loss 1.247843\n",
            "Epoch 51/300, loss 0.227582\n",
            "Epoch 101/300, loss 0.224382\n",
            "Epoch 151/300, loss 0.223228\n",
            "Epoch 201/300, loss 0.222127\n",
            "Epoch 251/300, loss 0.221042\n",
            "Epoch 300/300, loss 0.219991\n",
            "Saved numpy agent weights to ./models\n",
            "PyTorch device: cuda\n",
            "Constructed PyTorch agents from numpy weights. Agents ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "A3bn_ZJuxVFR",
        "outputId": "469b1c74-aca3-48ba-f408-03bde7571885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blackboard train] epoch 1/60, avg_loss=0.844182\n",
            "[Blackboard train] epoch 10/60, avg_loss=0.719792\n",
            "[Blackboard train] epoch 20/60, avg_loss=0.647405\n",
            "[Blackboard train] epoch 30/60, avg_loss=0.642161\n",
            "[Blackboard train] epoch 40/60, avg_loss=0.641224\n",
            "[Blackboard train] epoch 50/60, avg_loss=0.641392\n",
            "[Blackboard train] epoch 60/60, avg_loss=0.640584\n",
            "Running short batch training for blackboard (optional warm-start)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-116806523.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;31m# Optional: quick batch-train blackboard for a few epochs to initialize policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running short batch training for blackboard (optional warm-start)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0mtrain_blackboard_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblackboard_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Blackboard warm-start complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-243214650.py\u001b[0m in \u001b[0;36mtrain_blackboard_batch\u001b[0;34m(blackboard, epochs, lr, steps, batch_size)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_batch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mee_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0moptim_bb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0moptim_bb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# CELL 2/2\n",
        "# Blackboard implemented in PyTorch (differentiable).\n",
        "# - optional batch training of blackboard to minimize multi-step EE error\n",
        "# - run demo episode with finetune per-step (gradient-based)\n",
        "# - interactive widgets: Play/Slider, Random Target, Recompute, two bias sliders that modulate blackboard outputs in real-time\n",
        "\n",
        "# Install widgets if needed (Colab)\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output\n",
        "except Exception:\n",
        "    !pip install ipywidgets\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output\n",
        "\n",
        "# imports\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import time\n",
        "\n",
        "# ensure device from previous cell\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- Torch forward kinematics (differentiable) ----------\n",
        "LINKS_t = torch.tensor(LINKS, dtype=torch.float32, device=device)\n",
        "\n",
        "def fk_torch(q):\n",
        "    # q: (batch, DOF)\n",
        "    # returns ee: (batch,2)\n",
        "    angles = torch.cumsum(q, dim=-1)\n",
        "    x = torch.cumsum(torch.cos(angles) * LINKS_t, dim=-1)\n",
        "    y = torch.cumsum(torch.sin(angles) * LINKS_t, dim=-1)\n",
        "    ee = torch.stack([x[..., -1], y[..., -1]], dim=-1)\n",
        "    return ee\n",
        "\n",
        "# ---------- Blackboard PyTorch module ----------\n",
        "class BlackboardTorch(nn.Module):\n",
        "    def __init__(self, in_dim=2, hid=32, out_dim=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hid, hid),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hid, out_dim)\n",
        "        )\n",
        "    def forward(self, err):  # err: (batch,2)\n",
        "        z = self.net(err)\n",
        "        mod = 1.0 + torch.tanh(z)   # range (0,2)\n",
        "        return mod\n",
        "\n",
        "blackboard_t = BlackboardTorch().to(device)\n",
        "\n",
        "# ---------- Helper: convert numpy agent inputs to torch and run agents to get qdot (differentiable) ----------\n",
        "def agent_outputs_torch(q_batch, target_batch):\n",
        "    # q_batch: (batch, DOF)\n",
        "    # target_batch: (batch, 2)\n",
        "    # For per-joint agent networks, inputs are [q_i, tx, ty]\n",
        "    b = q_batch.shape[0]\n",
        "    q1 = q_batch[:,0:1]\n",
        "    q2 = q_batch[:,1:2]\n",
        "    inp1 = torch.cat([q1, target_batch], dim=-1).to(device).float()\n",
        "    inp2 = torch.cat([q2, target_batch], dim=-1).to(device).float()\n",
        "    out1 = agent1_torch(inp1)  # (batch,1)\n",
        "    out2 = agent2_torch(inp2)\n",
        "    out = torch.cat([out1, out2], dim=-1)  # (batch,2)\n",
        "    return out\n",
        "\n",
        "# ---------- Simulation (torch) for multiple steps (differentiable) ----------\n",
        "# --- REPLACEMENT: differentiable simulation + safe blackboard trainer ---\n",
        "\n",
        "import torch, torch.optim as optim\n",
        "\n",
        "# Ensure agent params are frozen (we do not train agents here)\n",
        "for p in agent1_torch.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in agent2_torch.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Ensure blackboard params are trainable\n",
        "for p in blackboard_t.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "def simulate_multi_step_torch(q0, target, blackboard, steps=20, dt=0.05, finetune_blackboard=False, optimizer=None):\n",
        "    \"\"\"\n",
        "    Fully differentiable simulation in torch.\n",
        "    q0: tensor (batch, DOF), target: tensor (batch,2)\n",
        "    returns: traj (steps+1, batch, DOF), mods (steps, batch, 2), errs (steps, batch)\n",
        "    All tensors kept as torch tensors (no detach) so gradients can flow to blackboard params.\n",
        "    \"\"\"\n",
        "    q = q0.clone()            # tensor\n",
        "    traj = [q]\n",
        "    mods = []\n",
        "    errs = []\n",
        "    for t in range(steps):\n",
        "        ee = fk_torch(q)                 # (batch,2)\n",
        "        err = target - ee                # (batch,2)\n",
        "        mod = blackboard(err)            # (batch,2) depends on blackboard params\n",
        "        out = agent_outputs_torch(q, target)  # (batch,2), depends on agent params but agents frozen\n",
        "        qdot = out * mod                 # (batch,2)\n",
        "        q = q + qdot * dt                # state updated; graph preserved\n",
        "        traj.append(q)\n",
        "        mods.append(mod)\n",
        "        errs.append(torch.norm(err, dim=-1))\n",
        "        # optional online finetune: if requested, perform optimizer step on blackboard to reduce immediate next-step error\n",
        "        if finetune_blackboard and (optimizer is not None):\n",
        "            optimizer.zero_grad()\n",
        "            ee_next = fk_torch(q)   # next EE (differentiable)\n",
        "            loss_step = torch.mean((target - ee_next)**2)\n",
        "            loss_step.backward()\n",
        "            optimizer.step()\n",
        "    traj = torch.stack(traj, dim=0)    # (steps+1, batch, DOF)\n",
        "    if len(mods)>0:\n",
        "        mods = torch.stack(mods, dim=0)    # (steps, batch, 2)\n",
        "    else:\n",
        "        mods = torch.zeros((0, q0.shape[0], 2), device=q0.device)\n",
        "    if len(errs)>0:\n",
        "        errs = torch.stack(errs, dim=0)    # (steps, batch)\n",
        "    else:\n",
        "        errs = torch.zeros((0, q0.shape[0]), device=q0.device)\n",
        "    return traj, mods, errs\n",
        "\n",
        "def train_blackboard_batch_torch(blackboard, epochs=100, lr=5e-3, steps=20, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train only blackboard parameters using fully-differentiable simulation.\n",
        "    Uses dataset Q, T from earlier (numpy).\n",
        "    \"\"\"\n",
        "    optimizer_bb = optim.Adam(blackboard.parameters(), lr=lr)\n",
        "    Q_np_local = Q   # make sure Q (numpy) exists from Cell1\n",
        "    T_np_local = T\n",
        "    n = Q_np_local.shape[0]\n",
        "    for ep in range(1, epochs+1):\n",
        "        perm = np.random.permutation(n)\n",
        "        total_loss = 0.0\n",
        "        for i in range(0, n, batch_size):\n",
        "            idx = perm[i:i+batch_size]\n",
        "            q_batch = torch.from_numpy(Q_np_local[idx]).to(device).float()      # (batch, DOF)\n",
        "            target_batch = torch.from_numpy(T_np_local[idx]).to(device).float() # (batch, 2)\n",
        "            # differentiable simulate\n",
        "            traj_t, mods_t, errs_t = simulate_multi_step_torch(q_batch, target_batch, blackboard, steps=steps, dt=0.05, finetune_blackboard=False, optimizer=None)\n",
        "            q_final = traj_t[-1]   # tensor still attached to graph\n",
        "            ee_final = fk_torch(q_final)\n",
        "            loss = torch.mean((target_batch - ee_final)**2)\n",
        "            optimizer_bb.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_bb.step()\n",
        "            total_loss += loss.item() * q_batch.size(0)\n",
        "        avg_loss = total_loss / n\n",
        "        if ep % 10 == 0 or ep==1 or ep==epochs:\n",
        "            print(f\"[Blackboard train] epoch {ep}/{epochs}, avg_loss={avg_loss:.6f}\")\n",
        "    return\n",
        "\n",
        "# ---- USAGE: call the corrected trainer instead of the old one ----\n",
        "# Example warm-start (short):\n",
        "train_blackboard_batch_torch(blackboard_t, epochs=60, lr=5e-3, steps=20, batch_size=128)\n",
        "\n",
        "\n",
        "# ---------- Prepare demo initial states (numpy) ----------\n",
        "initial_q_np = np.array([0.1, -0.2], dtype=np.float32)\n",
        "target_np = np.array([1.2, 0.2], dtype=np.float32)\n",
        "\n",
        "# Convert to torch for simulation\n",
        "q0_t = torch.from_numpy(initial_q_np[None,:]).to(device).float()\n",
        "target_t = torch.from_numpy(target_np[None,:]).to(device).float()\n",
        "\n",
        "# Optional: quick batch-train blackboard for a few epochs to initialize policy\n",
        "print(\"Running short batch training for blackboard (optional warm-start)...\")\n",
        "train_blackboard_batch(blackboard_t, epochs=60, lr=5e-3, steps=20, batch_size=128)\n",
        "print(\"Blackboard warm-start complete.\")\n",
        "\n",
        "# ---------- Demo: run a differentiable episode with optional finetune (online adaptation) ----------\n",
        "def run_demo_and_return_numpy(initial_q_np, target_np, blackboard, finetune_online=False, finetune_lr=1e-2, finetune_steps=1):\n",
        "    q0 = torch.from_numpy(initial_q_np[None,:]).to(device).float()\n",
        "    tgt = torch.from_numpy(target_np[None,:]).to(device).float()\n",
        "    # if online finetune, we create optimizer for blackboard\n",
        "    optim_bb = None\n",
        "    if finetune_online:\n",
        "        optim_bb = optim.Adam(blackboard.parameters(), lr=finetune_lr)\n",
        "    traj, mods, errs = simulate_multi_step(q0, tgt, blackboard, steps=300, dt=0.05, finetune_blackboard=finetune_online, finetune_steps=finetune_steps, optimizer=optim_bb)\n",
        "    # traj shape (steps+1, batch, DOF) -> squeeze batch\n",
        "    traj = traj[:,0,:]\n",
        "    if mods.shape[0]>0:\n",
        "        mods = mods[:,0,:]\n",
        "    errs = errs[:,0]\n",
        "    return traj, mods, errs\n",
        "\n",
        "# Quick run (no online finetune)\n",
        "traj_np, mods_np, errs_np = run_demo_and_return_numpy(initial_q_np, target_np, blackboard_t, finetune_online=False)\n",
        "print(\"Demo run complete: steps:\", traj_np.shape[0], \"final error:\", float(errs_np[-1]) if len(errs_np)>0 else 0.0)\n",
        "\n",
        "# ---------- Interactive visualization + UI ----------\n",
        "# We'll provide Play/Slider, Random Target, Recompute, and two sliders bias_m1 and bias_m2 that add offsets to blackboard output in real-time.\n",
        "\n",
        "# Prepare matplotlib figure (inline widget mode may be used in Colab)\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(9,5))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "ax.set_aspect('equal', 'box')\n",
        "lim = float(LINKS.sum() + 0.2)\n",
        "ax.set_xlim(-lim, lim); ax.set_ylim(-lim, lim)\n",
        "ax.set_title(\"2-DoF Arm (Agents + Blackboard)\")\n",
        "\n",
        "line, = ax.plot([], [], lw=3)\n",
        "pts, = ax.plot([], [], 'o')\n",
        "target_plot, = ax.plot([], [], marker='x', markersize=10, color='k')\n",
        "txt = ax.text(-lim+0.05, lim-0.15, '', fontsize=10)\n",
        "\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "line_m1, = ax2.plot([], [], label='m1')\n",
        "line_m2, = ax2.plot([], [], label='m2')\n",
        "ax2.legend()\n",
        "ax2.set_xlim(0, 300)\n",
        "ax2.set_ylim(0, 2.2)\n",
        "ax2.set_xlabel('Timestep')\n",
        "\n",
        "ax3 = fig.add_subplot(2,2,4)\n",
        "line_err, = ax3.plot([], [], color='r')\n",
        "ax3.set_xlim(0, 300)\n",
        "ax3.set_ylim(0, max(0.1, errs_np.max()*1.1 if len(errs_np)>0 else 0.1))\n",
        "ax3.set_xlabel('Timestep')\n",
        "\n",
        "# UI widgets\n",
        "play = widgets.Play(value=0, min=0, max=max(1, traj_np.shape[0]-1), step=1, interval=40)\n",
        "slider = widgets.IntSlider(value=0, min=0, max=max(1, traj_np.shape[0]-1), step=1)\n",
        "widgets.jslink((play, 'value'), (slider, 'value'))\n",
        "\n",
        "btn_random = widgets.Button(description='Random Target')\n",
        "btn_recompute = widgets.Button(description='Recompute Traj')\n",
        "btn_train_bb = widgets.Button(description='Train Blackboard (batch)')\n",
        "btn_toggle_finetune = widgets.ToggleButton(value=False, description='Online Finetune')\n",
        "bias_m1 = widgets.FloatSlider(min=-0.5, max=0.5, step=0.01, value=0.0, description='bias_m1')\n",
        "bias_m2 = widgets.FloatSlider(min=-0.5, max=0.5, step=0.01, value=0.0, description='bias_m2')\n",
        "status = widgets.Output()\n",
        "\n",
        "ui_top = widgets.HBox([play, slider, btn_random, btn_recompute, btn_train_bb, btn_toggle_finetune])\n",
        "ui_bot = widgets.HBox([bias_m1, bias_m2])\n",
        "display(ui_top, ui_bot, status)\n",
        "\n",
        "# functions to recompute trajectories with current blackboard and biases\n",
        "current_initial = initial_q_np.copy()\n",
        "current_target = target_np.copy()\n",
        "\n",
        "def compute_traj_with_biases():\n",
        "    # create a wrapper blackboard that adds biases to output (non-destructive)\n",
        "    def blackboard_with_bias(err_np):\n",
        "        # err_np: (batch,2) numpy\n",
        "        err_t = torch.from_numpy(err_np).to(device).float()\n",
        "        mod_t = blackboard_t(err_t).detach().cpu().numpy()\n",
        "        mod_t[:,0] += bias_m1.value\n",
        "        mod_t[:,1] += bias_m2.value\n",
        "        # clamp to [0,2]\n",
        "        mod_t = np.clip(mod_t, 0.0, 2.0)\n",
        "        return mod_t\n",
        "    # Instead of modifying blackboard internals, we will call the torch simulate but then multiply by bias factors externally:\n",
        "    # Simpler: run run_demo_and_return_numpy (which uses blackboard_t), then apply biases to mods sequence\n",
        "    traj, mods, errs = run_demo_and_return_numpy(current_initial, current_target, blackboard_t, finetune_online=btn_toggle_finetune.value)\n",
        "    # apply bias\n",
        "    if mods is not None and mods.shape[0]>0:\n",
        "        mods[:,0] = np.clip(mods[:,0] + bias_m1.value, 0.0, 2.0)\n",
        "        mods[:,1] = np.clip(mods[:,1] + bias_m2.value, 0.0, 2.0)\n",
        "    return traj, mods, errs\n",
        "\n",
        "# initial compute\n",
        "traj_np, mods_np, errs_np = compute_traj_with_biases()\n",
        "\n",
        "# drawing helpers\n",
        "def draw_frame(i):\n",
        "    i = int(i)\n",
        "    i = max(0, min(i, traj_np.shape[0]-1))\n",
        "    j = traj_np[i]\n",
        "    # compute joints via numpy fk\n",
        "    ee, joints = forward_kinematics_np(j[None,:])\n",
        "    joints = joints[0]\n",
        "    line.set_data(joints[:,0], joints[:,1])\n",
        "    pts.set_data(joints[:,0], joints[:,1])\n",
        "    target_plot.set_data([current_target[0]], [current_target[1]])\n",
        "    # update time plots\n",
        "    if mods_np is not None and mods_np.shape[0]>0:\n",
        "        line_m1.set_data(np.arange(min(i+1, mods_np.shape[0])), mods_np[:min(i+1, mods_np.shape[0]),0])\n",
        "        line_m2.set_data(np.arange(min(i+1, mods_np.shape[0])), mods_np[:min(i+1, mods_np.shape[0]),1])\n",
        "    if errs_np is not None and errs_np.shape[0]>0:\n",
        "        line_err.set_data(np.arange(min(i+1, errs_np.shape[0])), errs_np[:min(i+1, errs_np.shape[0])])\n",
        "    txt.set_text(f\"step={i}, err={errs_np[min(i,len(errs_np)-1)]:.3f}\" if len(errs_np)>0 else f\"step={i}\")\n",
        "    fig.canvas.draw_idle()\n",
        "\n",
        "# slider observer\n",
        "def on_slider_change(change):\n",
        "    draw_frame(change['new'])\n",
        "\n",
        "slider.observe(on_slider_change, names='value')\n",
        "\n",
        "# random target handler\n",
        "def on_random_clicked(b):\n",
        "    global current_target\n",
        "    ang = np.random.uniform(0, 2*np.pi)\n",
        "    r = np.random.uniform(0.2, LINKS.sum()*0.85)\n",
        "    current_target = np.array([r*np.cos(ang), r*np.sin(ang)], dtype=np.float32)\n",
        "    with status:\n",
        "        status.clear_output()\n",
        "        print(\"Random target:\", current_target)\n",
        "    recompute_traj(None)\n",
        "\n",
        "btn_random.on_click(on_random_clicked)\n",
        "\n",
        "# recompute handler\n",
        "def recompute_traj(b):\n",
        "    global traj_np, mods_np, errs_np\n",
        "    with status:\n",
        "        status.clear_output()\n",
        "        print(\"Recomputing trajectory (this may take a few seconds)...\")\n",
        "    traj_np, mods_np, errs_np = compute_traj_with_biases()\n",
        "    # update slider/play range\n",
        "    length = traj_np.shape[0]\n",
        "    play.max = max(0, length-1)\n",
        "    slider.max = max(0, length-1)\n",
        "    play.value = 0\n",
        "    slider.value = 0\n",
        "    with status:\n",
        "        print(\"Recomputed. steps:\", length, \"final err:\", float(errs_np[-1]) if len(errs_np)>0 else 0.0)\n",
        "\n",
        "btn_recompute.on_click(recompute_traj)\n",
        "\n",
        "# train blackboard batch handler\n",
        "def on_train_bb(b):\n",
        "    with status:\n",
        "        status.clear_output()\n",
        "        print(\"Training blackboard (batch) for 50 epochs (this may take time)...\")\n",
        "    train_blackboard_batch(blackboard_t, epochs=50, lr=5e-3, steps=20, batch_size=256)\n",
        "    with status:\n",
        "        print(\"Blackboard batch training finished. Recomputing trajectory...\")\n",
        "    recompute_traj(None)\n",
        "\n",
        "btn_train_bb.on_click(on_train_bb)\n",
        "\n",
        "# toggle online finetune: just recompute so it's used on next run\n",
        "def on_toggle_finetune(change):\n",
        "    with status:\n",
        "        status.clear_output()\n",
        "        print(\"Online finetune set to\", change['new'])\n",
        "    recompute_traj(None)\n",
        "\n",
        "btn_toggle_finetune.observe(on_toggle_finetune, names='value')\n",
        "\n",
        "# bias sliders: recompute on change\n",
        "def on_bias_change(change):\n",
        "    recompute_traj(None)\n",
        "\n",
        "bias_m1.observe(on_bias_change, names='value')\n",
        "bias_m2.observe(on_bias_change, names='value')\n",
        "\n",
        "# initial draw\n",
        "draw_frame(0)\n",
        "\n",
        "print(\"Interactive demo ready. Use controls to change target, train blackboard, toggle online finetune, and adjust bias sliders (bias affect modulation in realtime).\")\n"
      ]
    }
  ]
}